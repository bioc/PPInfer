%\VignetteIndexEntry{User manual}
\documentclass[a4paper]{article}

\usepackage{anysize}
\marginsize{1in}{1in}{0.5in}{0.5in}

\usepackage{setspace}
\linespread{1.5}

\usepackage{float}

\title{PPInfer: Inferring functionally related proteins using \\ protein interaction networks}
\author{Dongmin Jung, Xijin Ge}

\begin{document}
\SweaveOpts{concordance=TRUE}

\maketitle




\section{Introduction}

Interactions between proteins occur in many, if not most, biological processes. Most proteins perform their functions in networks associated with other proteins and other biomolecules. This fact has motivated the development of a variety of experimental methods for the identification of protein interactions. This variety has in turn ushered in the development of numerous different computational approaches for modeling and predicting protein interactions. Sometimes an experiment is aimed at identifying proteins closely related to some interesting proteins. A network based statistical learning method is used to infer the putative functions of proteins from the known functions of its neighboring proteins on a PPI network. This package identifies such proteins often involved in the same or similar biological functions.



\section{Graph}

Graph data is ubiquitous and graph mining is the study that aims to discover novel and insightful knowledge from data that is represented as a graph. Graph mining differs from traditional data mining in a number of critical ways. For example, the topic of classification in data mining is often introduced in relation to vector data; however, these techniques are often unsuitable when applied to graphs, which require an entirely different approach such as the use of graph kernels (Samatova \textit{et al.}, 2013). 

A support vector machine only applies to datasets in the real space. Often, however, we want to use a SVM on a dataset that is not a subset of the real space. This occurs in the case of biology and chemistry problems to describe our data. Fortunately, there is a ready solution to this problem, formalized in the use of kernel functions (Werther \& Seitz, 2008). We employ the kernel support vector machine (KSVM) based on the regularized Laplacian matrix (Smola \& Kondor, 2003) for a graph. The kernel matrix $K$ can now be used with a classification algorithm for predicting the class of vertices in the given dataset,
\begin{eqnarray*}
K=(I+\gamma L)^{-1},
\end{eqnarray*}
where $K$ is $N \times N$, $I$ is an identity matrix, $L$ is the normalized Laplacian matrix, and $\gamma$ is an appropriate decay constant. The decay constant is typically regarded as an arbitrary constant that is less than one.




\section{Support Vector Machine}

We focus on the application of computational method using a support vector machine. Suppose we have a dataset in the real space and that each point in our dataset has a corresponding class label. Our goal is to separate the points in our dataset according to their class label. A SVM is a linear binary classifier. The idea behind nonlinear SVM is to find an optimal separating hyperplane in high-dimensional feature space just as we did for the linear SVM in original space. At the heart of kernel methods is the notion of a kernel function. Broadly speaking, kernels can be thought of as functions that produce similarity matrices (Kolaczyk \& Csardi, 2014). One of the advantages of support vector machines is that we can improve performance by properly selecting kernels. In most applications, RBF kernels are widely used but kernels suited for specific applications are developed. Here, we select the graph kernel $K$ for PPI.

Data in many biological problems are often compounded by imbalanced class distribution, known as the imbalanced data problem, in which the size of one class is significantly larger than that of the other class. Many classification algorithms such as a SVM are sensitive to data with imbalanced class distribution, and result in a suboptimal classification. It is desirable to compensate the imbalance effect in model training for more accurate classification. One possible solution to the imbalanced data problem is to use one-class SVMs by learning from the target class only, instead of traditional binary SVMs. In one-class classification, it is assumed that only information of one of the classes, the target class, is available, and no information is available from the other class, known as the background. The task of one-class classification is to define a boundary around the target class such that it accepts as much of the targets as possible and excludes the outliers as much as possible (Ma, 2014).

However, one-class classifiers seldom outperform two-class classifiers when the data from two class are available (Ma, 2014). So the OCSVM and classical SVM are sequentially used in this package. First, we apply the OCSVM by training a one-class classifier using the data from the known class only. Let $n$ be the number of proteins in the target class. This model is used to identify distantly related proteins among remaining $N-n$ proteins in the background. Proteins with zero similarity with the target class are extracted. Then they are potentially defined as the other class by pseudo-absence selection methods (Senay \textit{et al.}, 2013) from spatial statistics. The target class can be seen as real presence data. For the data to be balanced, assume that two classes contain the same number of proteins. Next, by the classical SVM, these two classes are used to identify closely related proteins among remaining $N-2n$ proteins. Those found by this procedure can be functionally linked to the known class or interesting proteins.





\section{Example}

Consider a simple example about a graph representing the curated set of literature predicted protein-protein interactions, containing 2885 nodes, named using yeast standard names.

<<>>=
library(PPInfer)
data(litG)
litG <- graph_from_graphnel(litG)
summary(litG)
sg <- decompose(litG, min.vertices=50)
sg <- sg[[1]]           # largest subgraph
summary(sg)
@

We use only the largest subnetwork in this example. There are 88 proteins and 107 interactions.

<<>>=
V(sg)$color <- "green"
V(sg)$label.font <- 3
V(sg)$label.cex <- 1
V(sg)$label.color <- "black"
V(sg)[1:10]$color <- "blue"
@


\begin{figure}[H]
\centering
<<fig=TRUE, height=10, width=10>>=
plot(sg,layout=layout.kamada.kawai(sg),vertex.size=10)
@
\caption{Network among yeast proteins with target class in blue and remaining proteins in green.}
\end{figure}

First, calculate the kernel matrix and choose 10 proteins as a target class. Then we can find proteins closely related to the target class by using the KSVM for a graph (Samatova \textit{et al.}, 2013; Kolaczyk \& Csardi, 2014). Network of interactions among proteins with target class in blue and backgrounds in green. Red vertices represent the top 20 proteins which are most closely related to the target class.


<<>>=
K <- net.kernel(sg)
litG.infer <- net.infer(names(V(sg))[1:10],K,top=20)
index <- match(litG.infer$top,names(V(sg)))
V(sg)[index]$color <- "red"
@ 


\begin{figure}[H]
\centering
<<fig=TRUE, height=10, width=10>>=
plot(sg,layout=layout.kamada.kawai(sg),vertex.size=10)
@
\caption{Red vertices denote the top 20 yeast proteins which are most closely related to 10 proteins of the target class.}
\end{figure}


Note that the number of proteins is not greater than a half of total proteins in a kernel matrix due to $N-2n>0$. Also, the number of top proteins to be inferred is less than or equal to $N-2n$. If we use 50 proteins as a target class, then there is an error since $N-2n=-12$. If we use 40 proteins as a target, and want to find top 20 proteins, then the number of available top proteins are only 8, which is the minimum of $N-2n=8$ and 20.

<<echo=TRUE,eval=TRUE,results=verbatim>>=
litG.infer <- try(net.infer(names(V(sg))[1:50],K,top=20))
cat(litG.infer)
litG.infer <- net.infer(names(V(sg))[1:40],K,top=20)
litG.infer$top
@





\section{Protein-protein interaction}

We need a list of proteins and the kernel matrix to infer functionally related proteins. For the database for kernel matrix, we use the STRING data for protein-protein interactions for human and mouse.

<<eval=FALSE>>=
library(STRINGdb)

# human
string.db.9606 <- STRINGdb$new(version='10',species=9606)
string.db.9606.graph <- string.db.9606$get_graph()
K.9606 <- net.kernel(string.db.9606.graph)

# mouse
string.db.10090 <- STRINGdb$new(version='10',species=10090)
string.db.10090.graph <- string.db.10090$get_graph()
K.10090 <- net.kernel(string.db.10090.graph)
@

You do not have to calculate these kernel matrices. To save significant time, you can download kernel matrices at \texttt{http://ge-lab.org/dm/K9606.rds} for human and \texttt{http://ge-lab.org/dm/K10090.rds} for mouse.


\subsection{PPI for human}

Consider two examples for human. First, we can find proteins related to apoptosis. Then, load the kernel matrix for human.

<<eval=FALSE>>=
# load kernel matrix
# K.9606 <- readRDS(gzcon(url("http://ge-lab.org/dm/K9606.rds")))
K.9606 <- readRDS("K9606.rds")
# remove prefix
rownames(K.9606) <- sub('.*\\.','',rownames(K.9606))
colnames(K.9606) <- sub('.*\\.','',colnames(K.9606))

library(graph)
library(GOstats)
library(hgu95av2.db)

# load target class from KEGG apoptosis pathway
data(apopGraph)
list.proteins <- nodes(apopGraph)
head(list.proteins)
@

There are many types of protein ID or gene ID. By using \texttt{getBM} in \texttt{biomaRt}, we can change the format. For this reason, input and output formats must be available for \texttt{getBM}. Note that the number of proteins used as a target may be different from the number of proteins in the input since mapping between formats is not always one-to-one in \texttt{getBM}.

<<eval=FALSE>>=
# find top 100 proteins
apoptosis.infer <- ppi.infer.human(list.proteins,K.9606,output="entrezgene",100)
gene.id <- data.frame(apoptosis.infer$top)[,1]
head(gene.id)

# functional enrichment
params <- new("GOHyperGParams", geneIds=gene.id,annotation="org.Hs.eg.db",
             ontology="BP",pvalueCutoff=0.001,conditional=FALSE,
             testDirection="over")
(hgOver <- hyperGTest(params))

# top 10 categories
head(pvalues(hgOver),10)
head(select(GO.db, names(pvalues(hgOver)), "TERM", "GOID"),10)
@

We found functionally related top 100 proteins in terms of Entrez-ID by \texttt{ppi.infer.human}. However, we are often interested in biological functions about such inferred proteins. This is the top 10 categories from gene ontology. As we expected, inferred proteins have similar biological functions with the target, apoptosis. Thus this example supports that this model is reliable. Next, take another example. The protein p53 is known for inhibition of cancer. From KEGG pathway, we can find proteins for p53 signaling pathway. The procedure is the same to the previous example, but for the target class.


<<eval=FALSE>>=
library(KEGGgraph)
library(KEGG.db)

# load target class for p53
pName <- "p53 signaling pathway"
pId <- mget(pName, KEGGPATHNAME2ID)[[1]]
p53 <- system.file("extdata/hsa04115.xml",package="KEGGgraph")
p53graph <- parseKGML2Graph(p53,expandGenes=TRUE)
entrez <- translateKEGGID2GeneID(nodes(p53graph))
head(entrez)

# find top 100 proteins
p53.infer <- ppi.infer.human(entrez,K.9606,input="entrezgene",
                          output="entrezgene",100)
gene.id <- data.frame(p53.infer$top)[,1]
head(gene.id)

# functional enrichment
params <- new("GOHyperGParams", geneIds=gene.id,annotation="hgu95av2.db",
             ontology="BP",pvalueCutoff=0.001,conditional=FALSE,
             testDirection="over")
(hgOver <- hyperGTest(params))

# top 10 categories
head(pvalues(hgOver),10)
head(select(GO.db, names(pvalues(hgOver)), "TERM", "GOID"),10)
@



\subsection{PPI for mouse}

For mouse, we can infer functionally related proteins by \texttt{ppi.infer.mouse} with the kernel matrix for mouse. The first example is Acute myeloid leukemia.

<<eval=FALSE>>=
library(limma)
library(org.Mm.eg.db)

# load kernel matrix
# K.10090 <- readRDS(gzcon(url("http://ge-lab.org/dm/K10090.rds")))
K.10090 <- readRDS("K10090.rds")
# remove prefix
rownames(K.10090) <- sub('.*\\.','',rownames(K.10090))
colnames(K.10090) <- sub('.*\\.','',colnames(K.10090))

# load target class
mget('Acute myeloid leukemia', KEGGPATHNAME2ID)
kegg.mmu <- getGeneKEGGLinks(species.KEGG='mmu')
index <- match(kegg.mmu[,2],'path:mmu05221')
path.05221 <- 1:length(index)
index2 <- path.05221[index]
path.05221 <- path.05221[is.na(index2)==FALSE]
path.05221 <- kegg.mmu[path.05221,1]
head(path.05221)

# find top 100 proteins
path.05221.infer <- ppi.infer.mouse(path.05221,K.10090,
                                 input="entrezgene",output="entrezgene",100)
gene.id <- data.frame(path.05221.infer$top)[,1]
head(gene.id)

# functional enrichment
params <- new("GOHyperGParams", geneIds=gene.id,annotation="org.Mm.eg.db",
             ontology="BP",pvalueCutoff=0.001,conditional=FALSE,
             testDirection="over")
(hgOver <- hyperGTest(params))

# top 10 categories
head(pvalues(hgOver),10)
head(select(GO.db, names(pvalues(hgOver)), "TERM", "GOID"),10)
@

The second example is Ras signaling pathway. The Ras proteins are GTPases that function as molecular switches for signaling pathways regulating cell proliferation, survival, growth, migration, differentiation or cytoskeletal dynamism.

<<eval=FALSE>>=
# load target class
mget('Ras signaling pathway', KEGGPATHNAME2ID)
kegg.mmu <- getGeneKEGGLinks(species.KEGG='mmu')
index <- match(kegg.mmu[,2],'path:mmu04014')
path.04014 <- 1:length(index)
index2 <- path.04014[index]
path.04014 <- path.04014[is.na(index2)==FALSE]
path.04014 <- kegg.mmu[path.04014,1]
head(path.04014)

# find top 100 proteins
path.04014.infer <- ppi.infer.mouse(path.04014,K.10090,
                                 input="entrezgene",output="entrezgene",100)
gene.id <- data.frame(path.04014.infer$top)[,1]
head(gene.id)

# functional enrichment
params <- new("GOHyperGParams", geneIds=gene.id,annotation="org.Mm.eg.db",
             ontology="BP",pvalueCutoff=0.001,conditional=FALSE,
             testDirection="over")
(hgOver <- hyperGTest(params))

# top 10 categories
head(pvalues(hgOver),10)
head(select(GO.db, names(pvalues(hgOver)), "TERM", "GOID"),10)
@

We discussed about how to infer functionally realted proteins for human and mouse. Two functions \texttt{ppi.infer.human} and \texttt{ppi.infer.mouse} are specially designed because popular organisms are human and mouse. However, other kinds of species are also available in \texttt{net.infer} if kernel matrices are given.


\section{Session Information}

<<>>=
sessionInfo()
@



\section{References}

Kolaczyk, E. D. \& Csardi, G. (2014).
  \emph{Statistical analysis of network data with R}.
  Springer.
\bigbreak

\noindent Ma, Y. (2014).
  \emph{Support vector machines applications}.
  G. Guo (Ed.). Springer.
\bigbreak
  
\noindent Samatova, \textit{et al}. (Eds.). (2013).
  \emph{Practical graph mining with R}.
  CRC Press.
\bigbreak

\noindent Senay, S. D. \textit{et al}. (2013).
  Novel three-step pseudo-absence selection technique for improved species distribution modelling.
  \emph{PLOS ONE}.
  \textbf{8(8)}, e71218.
\bigbreak
  
\noindent Smola, A. J. \& Kondor, R. (2003).
  Kernels and regularization on graphs.
  \emph{In Learning theory and kernel machines}.
  144-158. Springer Berlin Heidelberg.
  
\noindent Werther, M., \& Seitz, H. (Eds.). (2008).
  \emph{Protein-protein interaction}.
  Springer.




\end{document}


